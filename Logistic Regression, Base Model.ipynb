{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfd5caa-a81b-4850-a753-ce14490cc1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'data' has no attribute 'set_option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m nan\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#pd.set_option('display.max_rows', None)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#pd.set_option('display.max_columns', 100)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'data' has no attribute 'set_option'"
     ]
    }
   ],
   "source": [
    "#import panda \n",
    "import data as pd\n",
    "import math as math\n",
    "\n",
    "#remove Nan entries\n",
    "nan= float ('nan')\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e1dd3-e1b2-433e-af9c-bc181e640643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb956b-eabf-42c6-ad54-226545135cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22afe8-938a-44ff-ad6e-0f04934f4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the ucimlrepo\n",
    "%pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e438d84-6ab8-4c56-9c48-64da860e5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "diabetes_data = fetch_ucirepo(id=296) \n",
    "  \n",
    "# features and ids as pandas DataFrames\n",
    "data_ids = diabetes_data.data.ids\n",
    "data_features = diabetes_data.data.features\n",
    "data_target = diabetes_data.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b47a6-821b-446d-a6f4-4d5ad2a5294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\kehin\\Downloads\\diabetes+130-us+hospitals+for+years+1999-2008\\diabetic_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce6661-a220-4de9-810a-409d53bb4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dummy column to merge\n",
    "data_ids.loc[:, 'merge_temp'] = data_ids.index\n",
    "data_features.loc[:, 'merge_temp'] = data_features.index\n",
    "data_target.loc[:, 'merge_temp'] = data_target.index\n",
    "\n",
    "# merge into one DataFrame\n",
    "data_temp = data_ids.merge(data_features, on='merge_temp')\n",
    "\n",
    "data_raw = data_temp.merge(data_target, on='merge_temp')\n",
    "\n",
    "del data_temp\n",
    "\n",
    "data_raw = data_raw.drop(columns = 'merge_temp')\n",
    "\n",
    "print(data_raw.columns, '\\n')\n",
    "\n",
    "print(data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640f24d-26b2-4855-8f28-5d4e8327fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available columns\n",
    "print(df.columns)\n",
    "\n",
    "# Define your target column (for example, predicting readmission)\n",
    "target_col = 'readmitted'  # or 'readmit_binary' if you already created it\n",
    "\n",
    "# Create features (X) and target (y)\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# If you're going to merge again after preprocessing, use df.merge() carefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050a2ba-2123-49e7-9933-ab0c5cea2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing/unnecesary features\n",
    "dropped_columns = ['encounter_id', 'weight', 'payer_code', 'medical_specialty']\n",
    "\n",
    "data_raw = data_raw.drop(columns = dropped_columns)\n",
    "\n",
    "print(data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe285ef3-a61f-43b8-b603-4a1fab6101dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = ['encounter_id', 'weight', 'payer_code', 'medical_specialty']\n",
    "data_raw = data_raw.drop(columns=dropped_columns)\n",
    "\n",
    "print(data_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cc654-6689-40c2-92fc-c672e9371f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame named data_raw\n",
    "file_path = r'C:\\Users\\kehin\\Downloads\\diabetes+130-us+hospitals+for+years+1999-2008\\diabetic_data.csv'\n",
    "data_raw = pd.read_csv(file_path)\n",
    "\n",
    "# Now drop the unwanted columns\n",
    "dropped_columns = ['encounter_id', 'weight', 'payer_code', 'medical_specialty']\n",
    "data_raw = data_raw.drop(columns=dropped_columns)\n",
    "\n",
    "print(data_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e1f9b-bc6d-488b-89f3-9097e61df891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace NaNs with -1\n",
    "data_raw = data_raw.replace(np.nan, -1)\n",
    "\n",
    "# Drop rows where race is missing (encoded as -1)\n",
    "data_raw = data_raw.drop(data_raw.loc[data_raw['race'] == -1].index)\n",
    "\n",
    "# Replace missing diagnosis and lab results encoded as -1 with 'N/A'\n",
    "cols_to_replace = ['diag_1', 'diag_2', 'diag_3', 'A1Cresult', 'max_glu_serum']\n",
    "data_raw[cols_to_replace] = data_raw[cols_to_replace].replace(-1, 'N/A')\n",
    "\n",
    "print(data_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9916dc2-052d-4253-8ad4-db59c12c1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace NaN values with -1\n",
    "diabetes_df = diabetes_df.replace(np.nan, -1)\n",
    "\n",
    "# Drop rows where 'race' is missing (encoded as -1)\n",
    "diabetes_df = diabetes_df.drop(diabetes_df[diabetes_df['race'] == -1].index)\n",
    "\n",
    "# Replace missing diagnosis and lab result values (-1) with 'N/A'\n",
    "cols_to_replace = ['diag_1', 'diag_2', 'diag_3', 'A1Cresult', 'max_glu_serum']\n",
    "diabetes_df[cols_to_replace] = diabetes_df[cols_to_replace].replace(-1, 'N/A')\n",
    "\n",
    "print(diabetes_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61852e7-a9ff-4bed-8b90-3be33529faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace NaN values with -1\n",
    "df = df.replace(np.nan, -1)\n",
    "\n",
    "# Drop rows where 'race' is missing (encoded as -1)\n",
    "df = df.drop(df[df['race'] == -1].index)\n",
    "\n",
    "# Replace missing diagnosis and lab result values (-1) with 'N/A'\n",
    "cols_to_replace = ['diag_1', 'diag_2', 'diag_3', 'A1Cresult', 'max_glu_serum']\n",
    "df[cols_to_replace] = df[cols_to_replace].replace(-1, 'N/A')\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad259bc4-88e1-41ca-a6ee-82784f700543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['encounter_id', 'weight', 'payer_code', 'medical_specialty']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Replace NaN with -1\n",
    "df = df.replace(np.nan, -1)\n",
    "\n",
    "# Drop rows where 'race' is missing (encoded as -1)\n",
    "df = df.drop(df[df['race'] == -1].index)\n",
    "\n",
    "# Replace missing diagnosis and lab values with 'N/A'\n",
    "cols_to_replace = ['diag_1', 'diag_2', 'diag_3', 'A1Cresult', 'max_glu_serum']\n",
    "df[cols_to_replace] = df[cols_to_replace].replace(-1, 'N/A')\n",
    "\n",
    "# Final shape\n",
    "print(df.shape)  # Should print (99493, 46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f5658-0e6a-4a1f-9d3e-31c9582dfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in 'race' before cleaning\n",
    "print(df['race'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ac276-3838-43ad-880d-547b78f4f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unneeded columns\n",
    "columns_to_drop = ['encounter_id', 'weight', 'payer_code', 'medical_specialty']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Remove rows with missing race\n",
    "df = df[df['race'] != '?']  # Fix is here!\n",
    "\n",
    "# Replace remaining NaNs with -1\n",
    "df = df.replace(np.nan, -1)\n",
    "\n",
    "# Replace specific columns' missing values with 'N/A'\n",
    "cols_to_replace = ['diag_1', 'diag_2', 'diag_3', 'A1Cresult', 'max_glu_serum']\n",
    "df[cols_to_replace] = df[cols_to_replace].replace(-1, 'N/A')\n",
    "\n",
    "# Check shape\n",
    "print(df.shape)  # This should now print (99493, 46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596a8fe-1224-4949-9463-3821c66cb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records of patients who expired or were discharged to hospice care\n",
    "# Reference: discharge disposition ID codes\n",
    "\n",
    "expired_or_hospice_ids = [\n",
    "    11,  # Expired\n",
    "    13,  # Hospice/home\n",
    "    14,  # Hospice/medical facility\n",
    "    19,  # Expired at home (Medicaid only, hospice)\n",
    "    20,  # Expired in a medical facility (Medicaid only, hospice)\n",
    "    21   # Expired, place unknown (Medicaid only, hospice)\n",
    "]\n",
    "\n",
    "df = df[~df['discharge_disposition_id'].isin(expired_or_hospice_ids]]\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb7053-f428-4eef-8c1c-a233e4f06b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['discharge_disposition_id'].isin(expired_or_hospice_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa63f84-6c4d-4696-9f44-ce536d4fc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "expired_or_hospice_ids = [\n",
    "    11,  # Expired\n",
    "    13,  # Hospice/home\n",
    "    14,  # Hospice/medical facility\n",
    "    19,  # Expired at home (Medicaid only, hospice)\n",
    "    20,  # Expired in a medical facility (Medicaid only, hospice)\n",
    "    21   # Expired, place unknown (Medicaid only, hospice)\n",
    "]\n",
    "\n",
    "df = df[~df['discharge_disposition_id'].isin(expired_or_hospice_ids)]\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca321ef-3553-4506-be16-6c81184dd7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cleaned (no missing values) dataset to file\n",
    "df.to_csv('./data/dataset_clean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0349bd3-26bd-4b70-b7ae-efee930f43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the 'data' folder if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Now save the dataframe\n",
    "df.to_csv('./data/dataset_clean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd9414f-6096-4b69-8946-808ff9ac5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6df1d5-03fb-4b29-8194-43a9a0a5b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Step 1: Select Features and Target ---\n",
    "df['readmit_30'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "X = df.drop(columns=['readmitted', 'readmit_30'])  # Drop target and redundant original column\n",
    "y = df['readmit_30']\n",
    "\n",
    "# --- Step 2: Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Step 3: Preprocessing ---\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Step 4: Define Base Model Pipeline ---\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "# --- Step 5: Fit Model ---\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 6: Evaluate Model ---\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481ad8c-13c4-4bd4-9e11-51d8b3229283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# STEP 1: Load the cleaned dataset\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# STEP 2: Create binary target variable\n",
    "# Make sure this is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b0f1b-e3b9-49b8-9331-809aa33a3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Load cleaned dataset\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# 2. Convert target to binary classification: 1 if readmitted within 30 days, else 0\n",
    "df['readmit_30'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# 3. Drop original 'readmitted' column\n",
    "df = df.drop('readmitted', axis=1)\n",
    "\n",
    "# 4. Separate features and target\n",
    "X = df.drop('readmit_30', axis=1)\n",
    "y = df['readmit_30']\n",
    "\n",
    "# 5. Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 6. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 7. Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 8. Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')  # 'balanced' helps with class imbalance\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 10. Evaluate performance\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d7d9f-09a4-4f7c-9d36-18fdc737eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cc59e-f19b-4aee-84a6-7a9a0e6a826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076fa42-8c80-4271-bdc8-d1fcafaa8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a2f93-b833-4313-9356-7b9e36efddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5bb585-cff4-4010-91ed-aad8cfc30d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True).plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6a5b4-bf27-4db3-97a8-694fc1f2e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the cleaned dataset (just to access the 'race' column for test set)\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# If you already split data, get race column for test set (assuming same indices)\n",
    "race_test = df.loc[X_test.index, 'race']\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "audit_df = pd.DataFrame({\n",
    "    'race': race_test,\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred\n",
    "})\n",
    "\n",
    "# Get unique races\n",
    "races = audit_df['race'].unique()\n",
    "\n",
    "# Run classification report per race group\n",
    "for group in races:\n",
    "    print(f\"\\nPerformance for Race Group: {group}\")\n",
    "    subset = audit_df[audit_df['race'] == group]\n",
    "    print(classification_report(subset['actual'], subset['predicted'], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bddc0-e787-44f9-9676-820ccb09f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load your full cleaned dataset\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['readmitted'])  # Adjust if your target column is named differently\n",
    "y = df['readmitted']\n",
    "race = df['race']  # Save race column separately\n",
    "\n",
    "# Split the data and race together to retain alignment\n",
    "X_train, X_test, y_train, y_test, race_train, race_test = train_test_split(\n",
    "    X, y, race, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7fedf-4068-47a9-88e1-e82c502571ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cabb6-b00f-4333-b534-dc1a78822480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# Target column\n",
    "y = df['readmitted']\n",
    "X = df.drop(columns=['readmitted'])\n",
    "\n",
    "# Save race separately before encoding\n",
    "race = X['race']\n",
    "\n",
    "# One-hot encode categorical variables (like 'race', 'gender', etc.)\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the encoded dataset while keeping race column aligned\n",
    "X_train, X_test, y_train, y_test, race_train, race_test = train_test_split(\n",
    "    X_encoded, y, race, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5633037-26ad-47cd-8a82-29b4495ebbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944c630-654e-4916-9605-712293a7b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load the cleaned dataset\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# 2. Separate features and target\n",
    "y = df['readmitted']\n",
    "X = df.drop(columns=['readmitted'])\n",
    "\n",
    "# 3. Save race column before encoding for bias audit later\n",
    "race = X['race']\n",
    "\n",
    "# 4. One-hot encode categorical variables (drop_first=True to avoid dummy variable trap)\n",
    "X_encoded = pd.get_dummies(X.drop(columns=['race']), drop_first=True)\n",
    "\n",
    "# 5. Split into train and test sets, stratifying on target y\n",
    "X_train, X_test, y_train, y_test, race_train, race_test = train_test_split(\n",
    "    X_encoded, y, race, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 6. Initialize and train Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 7. Predict on test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# 8. Print overall evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# 9. Bias audit by race group\n",
    "audit_df = pd.DataFrame({\n",
    "    'race': race_test.values,\n",
    "    'actual': y_test.values,\n",
    "    'predicted': y_pred\n",
    "})\n",
    "\n",
    "print(\"\\nBias Audit by Race:\")\n",
    "for group in audit_df['race'].unique():\n",
    "    print(f\"\\nRace Group: {group}\")\n",
    "    subset = audit_df[audit_df['race'] == group]\n",
    "    print(classification_report(subset['actual'], subset['predicted'], zero_division=0))\n",
    "\n",
    "# 10. (Optional) Visualize Confusion Matrix with Seaborn heatmap\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9d5fd-41ed-43d2-9817-048d8d28ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed7318-d375-4f7a-8802-0f604bc56d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['readmitted'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770b1cf-f25e-43fb-93b3-f53fd8faa7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readmit_binary'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcbaae-c921-4b39-9352-7b2436e775d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['readmitted', 'readmit_binary'])  # drop both old and new target\n",
    "y = df['readmit_binary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfb84f-49b7-4111-a7d3-a908b3db29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc123a-1ae0-44ba-9202-1cf628ea3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary column: 1 if readmitted <30 days, else 0\n",
    "df['readmit_binary'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299fd99-ce45-47b5-bc4c-1f6f6b8ce6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=['readmitted', 'readmit_binary'])  # drop original target\n",
    "y = df['readmit_binary']\n",
    "\n",
    "# Train-test split (stratify ensures class balance in test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression with class balancing\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3d83e-1a95-401c-aca6-10fdeb8ab6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target and any identifiers\n",
    "X = df.drop(columns=['readmitted', 'readmit_binary', 'encounter_id', 'patient_nbr'])  # if these exist\n",
    "\n",
    "# One-hot encode all categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Double-check that all features are numeric\n",
    "print(X.dtypes)  # should all be int or float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed1940-0b2b-4d15-9a69-841ebd651f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['readmitted', 'readmit_binary', 'encounter_id', 'patient_nbr']\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "X = df.drop(columns=existing_columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b94fa-bfd9-435b-bea8-8f382c5eee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Confirm everything is numeric\n",
    "print(X.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722259f-227a-47f5-a0b0-21f77d00c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef839cd-ea97-49ee-955a-5bed88a581a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression with class balancing\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335d980-fbf3-48b1-8dd5-d435622fdf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Proceed with train-test split and model fitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76660aed-0937-4512-8fc6-4d263659fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=5000, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bde54b-b967-42ed-810a-42ff9cac3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "logreg = LogisticRegression(max_iter=5000, solver='saga', class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae7f4a-667e-495f-9a17-cd323a4c9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=5000,           # increased iterations\n",
    "    solver='saga',           # better for large datasets and sparse data\n",
    "    class_weight='balanced'  # handle class imbalance\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a55a8-2d9a-432e-a632-23a28304990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759e29e-3ebd-47b9-b78e-3cb547a93ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you include print statements\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d041003-2c8f-4677-96fe-0d69ee007e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "logreg = LogisticRegression(max_iter=5000, solver='saga', class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Output\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cf091f2-59b1-4394-a10a-96614601494c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, stratify\u001b[38;5;241m=\u001b[39my, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c98df9e1-5a86-4e55-8794-a12c17eef030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97109, 46)\n",
      "Index(['patient_nbr', 'race', 'gender', 'age', 'admission_type_id',\n",
      "       'discharge_disposition_id', 'admission_source_id', 'time_in_hospital',\n",
      "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
      "       'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1',\n",
      "       'diag_2', 'diag_3', 'number_diagnoses', 'max_glu_serum', 'A1Cresult',\n",
      "       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
      "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
      "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
      "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
      "       'glyburide-metformin', 'glipizide-metformin',\n",
      "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
      "       'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('./data/dataset_clean.csv')\n",
    "\n",
    "# Quick check\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7ceda60-98dd-4ae0-9891-da56a55d537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (77687, 2319), Test shape: (19422, 2319)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is already loaded\n",
    "\n",
    "# Target variable\n",
    "y = df['readmitted']\n",
    "\n",
    "# Drop target and identifiers from features\n",
    "X = df.drop(columns=['readmitted', 'patient_nbr'])\n",
    "\n",
    "# One-hot encode categorical variables (assuming some are categorical)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa20e8b8-74cd-4bb3-82b8-a89ddf15450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 926  673  626]\n",
      " [2084 2534 2375]\n",
      " [2013 2423 5768]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         <30       0.18      0.42      0.26      2225\n",
      "         >30       0.45      0.36      0.40      6993\n",
      "          NO       0.66      0.57      0.61     10204\n",
      "\n",
      "    accuracy                           0.48     19422\n",
      "   macro avg       0.43      0.45      0.42     19422\n",
      "weighted avg       0.53      0.48      0.49     19422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize logistic regression with class_weight balanced and enough iterations\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4201f69-a805-4ea9-b715-c90091c77540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
